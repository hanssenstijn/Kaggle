---
title: BagofWordsMeetsBagsofPopcorn
author: Stijn Hanssen
date: 4 December 2018
output: html_document
---

### Clear working space
First step is to make sure that the working space is empty.
```{r}
# clear all variables
rm(list=ls()) 
```

### Load packages
Loading the required packages for specific functions.
```{r}
if (!require("pacman")) suppressPackageStartupMessages(install.packages("pacman"))
pacman::p_load(dplyr,tm,knitr,RWeka,randomForest)
```

### Set working directory
Choose the path were the data-sets are stored
```{r}
DATA.DIR <- "~/GitHub/Kaggle/BagofWordsMeetsBagsofPopcorn"
setwd(DATA.DIR)
# See which files are in WD
list.files() 
```

### Import data
The training and test set will be imported into R.
```{r}
word_train = read.table("labeledTrainData.tsv", header = T)
word_test = read.table("testData.tsv", header = T)
full = bind_rows(word_train,word_test)

dim(word_train)
dim(word_test)
str(full)
```

### Inspect a review
In the text are the line breaks shown as *<br />*. This have to be taken care off. Furthermore, we are not interested in the numberws and punctuations. In addition, we will remove the common stop words, convert the words back to their roots. Lastly, we will remove all the whitespaces.
```{r}
as.character(full$review[[1]])
```

### Pre-data cleaning
gsub() function replaces all matches of a string, if the parameter is a string vector, returns a string vector of the same length and with the same attributes.
```{r}
full$review = gsub('<br />',' ',full$review)
as.character(full$review[[1]])
```

### Converting into a corpus
Tm package text mining in R. Main structure managing documents are Corpus. 
```{r}
text = VCorpus(VectorSource(full$review))
```

### Data cleaning the corpus
Transforming all letter to lowercases, remove the numbers, get rid of the punctuations, delete stopwords, use the stem function to get the root words and lastly delete the whitespaces
```{r}
text = tm_map(text, content_transformer(tolower))
as.character(full$review[[1]])
text = tm_map(text,removeNumbers)
as.character(full$review[[1]])
text = tm_map(text,removePunctuation)
as.character(full$review[[1]])
text = tm_map(text,removeWords,stopwords())
as.character(full$review[[1]])
text = tm_map(text,stemDocument)
as.character(full$review[[1]])
text = tm_map(text,stripWhitespace)
as.character(full$review[[1]])
```

# N grams Tokenizer
The bigramTokenizer will be used for word combination of two words. while the trygramTokenizer will be used for the combination of three words.
```{r}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
```

### Apply tokenization
```{r}
dtm1 <- tm::DocumentTermMatrix(text)
dtm1 <- removeSparseTerms(dtm1,0.95)

dtm2 <- tm::DocumentTermMatrix(text, control=list(tokenize=BigramTokenizer))
dtm2 <- removeSparseTerms(dtm2,0.99)

dtm3 <- tm::DocumentTermMatrix(text, control=list(tokenize=TrigramTokenizer))
dtm3 <- removeSparseTerms(dtm3,0.999)
```

### Transform into DF
```{r}
dataset1 = as.data.frame(as.matrix(dtm1))
head(dataset1)
dataset2 = as.data.frame(as.matrix(dtm2))
head(dataset2)
dataset3 = as.data.frame(as.matrix(dtm3))
head(dataset3)
```

### Determine the most often occuring words
```{r}
dataset_counts1 = as.data.frame(colSums(dataset1))
head(dataset_counts1)
dataset_counts1$word = rownames(dataset_counts1)
head(dataset_counts1)
colnames(dataset_counts1) = c("count","word")
head(dataset_counts1)
dataset_counts1 = dataset_counts1[c(2,1)] 
head(dataset_counts1)
dataset_counts1 = dataset_counts1 %>% arrange(-count)
head(dataset_counts1)

dataset_counts2 = as.data.frame(colSums(dataset2))
dataset_counts2$word = rownames(dataset_counts2)
colnames(dataset_counts2) = c("count","word")
dataset_counts2 = dataset_counts2[c(2,1)] 
dataset_counts2 = dataset_counts2 %>% arrange(-count)

dataset_counts3 = as.data.frame(colSums(dataset3))
dataset_counts3$word = rownames(dataset_counts3)
colnames(dataset_counts3) = c("count","word")
dataset_counts3 = dataset_counts3[c(2,1)] 
dataset_counts3= dataset_counts3 %>% arrange(-count)

head(dataset_counts2,20)
head(dataset_counts3,20)
```

