---
title: "Bag of Words meets Bag of Popcorn"
output: github_document
---

### Clear working space
First step is to make sure that the working space is empty.

```{r}
# clear all variables
rm(list=ls()) 
```

### Load libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F,message = F)

library(dplyr)
library(tm)
library(knitr)
library(RWeka)
library(randomForest)
```

### Set working directory
Choose the path were the data-sets are stored
```{r}
DATA.DIR <- "~/GitHub/Kaggle/BagofWordsMeetsBagsofPopcorn"
setwd(DATA.DIR)
# See which files are in WD
list.files() 
```

# Project Briefing
- We use the *Bag of Words meets Bag of Popcorn* dataset from kaggle to do a sentiment analysis of movies based on user reviews. We deal with *unstructered data* in the form of written text making use of *Natural Language Processing* libraries such as *tm* and *Rweka*. Then we apply Machine Learning algorithms such as *Logistic Regression* & *Random Forest* to predict viewer sentiment on test dat.

Training Dataset- We use the "Bag of Words meets Bag of Popcorrn" dataset from kaggle to train the algorithm.
- Data Import
```{r}

word_train = read.table("labeledTrainData.tsv", header = T)
word_test = read.table("testData.tsv", header = T)


full = bind_rows(word_train,word_test)
dim(word_train)
dim(word_test)
str(full)
```

# Data Pre-Processing
- We see that the review column in our data set is the variable of interest.
- sentiment column provides the label for the training algorithm
- We start cleaning the text
```{r}
as.character(full$review[[1]])

full$review = gsub(full$review, pattern = '<br />', replacement = ' ')

text = VCorpus(VectorSource(full$review))        # Creating a Corpus of reviews

text = tm_map(text,content_transformer(tolower))  # Converting to lower case

as.character(text[[1]])

text = tm_map(text,removeNumbers)                # Removing numbers

as.character(text[[1]])
# We can see that all text is now in lower case and numbers have been removed.

text = tm_map(text,removePunctuation)  # Removing Punctuations

as.character(text[[1]])
```

# Cleaning the text - Step 2
- We will use a package *SnowballC* to clean the data further.
```{r}
text = tm_map(text,removeWords,stopwords())      #to remove common words

as.character(text[[1]])

text = tm_map(text,stemDocument)                 #to convert words back to root words.

as.character(text[[1]])

text = tm_map(text,stripWhitespace)              #to remove white spaces

as.character(text[[1]])
```

- stopwords() contains a list of irrelevant words in English which usually doesnot help the algorithm learn about the reviews.

- stemDocument helps in getting the root in each word so that we can train the model using just one version of the same words leading to much better accuracy.

- removing white spaces becomes important at this point as we have removed and transformed some of the words. It will bring uniformity and make sure that only the words are retained, no empty spaces before or after them.

# N grams Tokenizer
- We will now tokenize the words from the individual reviews and make coulmn of all the individual unique words. This is done using a Document term Matrix or DTM.
- But single words in themselves maynot be as powerful in suggeting the review. We also take phrases - 2 word & 3 word phrases for better prominence of the terms indicating the sentiment of reviews.
eg.s can be - "best movie ever" , "worst movie" etc.

```{r}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
```

- We create a BigramTokenizer and a TrigramTokenizer containing 2 word
and 3 word phrases respectively using *Rweka* package.

- Creating the DTMs for 1-grams, 2-grams & 3-grams
```{r}
## this is dtm for 1 words. we will find the more important words and just use those
dtm1 = DocumentTermMatrix(text)
dtm1 = removeSparseTerms(dtm1,0.95)
dtm1

## similarly, dtm of 2 words
dtm2 = DocumentTermMatrix(text, control = list(tokenize = BigramTokenizer))
dtm2 = removeSparseTerms(dtm2,0.99)
dtm2

##dtm for 3 words
dtm3 = DocumentTermMatrix(text, control = list(tokenize = TrigramTokenizer))
dtm3 = removeSparseTerms(dtm3,0.999)
dtm3
```

# Seeing the final DTMs
- We will also see the most frequently occuring 1 word , 2-word & 3-word phrases.
```{r}
dataset1 = as.data.frame(as.matrix(dtm1))
head(dataset1)
dataset2 = as.data.frame(as.matrix(dtm2))
head(dataset2)
dataset3 = as.data.frame(as.matrix(dtm3))
head(dataset3)

#for dtm1
dataset_counts1 = as.data.frame(colSums(dataset1))
head(dataset_counts1)
dataset_counts1$word = rownames(dataset_counts1)
head(dataset_counts1)

colnames(dataset_counts1) = c("count","word")
head(dataset_counts1)
dataset_counts1 = dataset_counts1[c(2,1)] 
head(dataset_counts1)
dataset_counts1 = dataset_counts1 %>% arrange(-count)
head(dataset_counts1)

#for dtm2
dataset_counts2 = as.data.frame(colSums(dataset2))
dataset_counts2$word = rownames(dataset_counts2)

colnames(dataset_counts2) = c("count","word")
dataset_counts2 = dataset_counts2[c(2,1)] 
dataset_counts2 = dataset_counts2 %>% arrange(-count)

#for dtm3
dataset_counts3 = as.data.frame(colSums(dataset3))
dataset_counts3$word = rownames(dataset_counts3)

colnames(dataset_counts3) = c("count","word")
dataset_counts3 = dataset_counts3[c(2,1)] 
dataset_counts3= dataset_counts3 %>% arrange(-count)

head(dataset_counts1,20)
head(dataset_counts2,20)
head(dataset_counts3,20)
```



#Model fitting

```{r}
#Pre processing
final_dataset_words = bind_rows(dataset_counts1,dataset_counts2,dataset_counts3) 
final_dataset = as.data.frame(cbind(dataset1,dataset2,dataset3))

dataset_train = final_dataset[1:25000,]
dataset_test = final_dataset[25001:50000,]

```

- We build a random forest classifier
- Next, we also see which are the most important words accoring to the model. We have a look at top 200 such words.

# Model fitting - Logistic Regression

```{r}

dataset_train$y_pred = word_train$sentiment
dataset_train$y_pred = as.factor(dataset_train$y_pred)
m1 = glm(formula = y_pred ~.,
         data = dataset_train,
         family = 'binomial')
pred = predict(m1,newdata = dataset_test,type = "response")
probs <- exp(pred)/(1+exp(pred)) 
#gives you probability that y=1 for each observation

```
