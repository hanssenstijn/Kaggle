knitr::opts_chunk$set(echo = TRUE, warning = F,message = F)
library(dplyr)
library(tm)
library(knitr)
library(RWeka)
library(randomForest)
setwd("~/GitHub/Kaggle/BagofWordsMeetsBagsofPopcorn")
# clear all variables
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, warning = F,message = F)
library(dplyr)
library(tm)
library(knitr)
library(RWeka)
library(randomForest)
DATA.DIR <- "~/GitHub/Kaggle/BagofWordsMeetsBagsofPopcorn"
setwd(DATA.DIR)
# See which files are in WD
list.files()
word_train = read.table("labeledTrain.tsv", header = T)
word_train = read.table("labeledTrainData.tsv", header = T)
word_test = read.table("testData.tsv", header = T)
full = bind_rows(word_train,word_test)
dim(word_train)
dim(word_test)
str(full)
View(word_test)
View(word_train)
View(full)
full[1]
full[[1]]
full(1,1)
full(1,1)
full[1,1]
full(1,)
View(full)
full[[3]]
full(1,3)
word_train(1,3)
word_train[1,3]
full$review = gsub(full$review, pattern = '<br />', replacement = ' ')
text = VCorpus(VectorSource(full$review))        # Creating a Corpus of reviews
text = tm_map(text,content_transformer(tolower))  # Converting to lower case
text = tm_map(text,removeNumbers)                # Removing numbers
as.character(text[[1]])
# We can see that all text is now in lower case and numbers have been removed.
text = tm_map(text,removePunctuation)            # Removing Punctuations
full$review = gsub(full$review, pattern = '<br />', replacement = ' ')
text = VCorpus(VectorSource(full$review))        # Creating a Corpus of reviews
as.character(text[[1]])
full$review = gsub(full$review, pattern = '<br />', replacement = ' ')
text = VCorpus(VectorSource(full$review))        # Creating a Corpus of reviews
text = tm_map(text,content_transformer(tolower))  # Converting to lower case
text = tm_map(text,removeNumbers)                # Removing numbers
as.character(text[[1]])
# We can see that all text is now in lower case and numbers have been removed.
text = tm_map(text,removePunctuation)            # Removing Punctuations
full$review = gsub(full$review, pattern = '<br />', replacement = ' ')
text = VCorpus(VectorSource(full$review))        # Creating a Corpus of reviews
text = tm_map(text,content_transformer(tolower))  # Converting to lower case
text = tm_map(text,removeNumbers)                # Removing numbers
as.character(text[[1]])
# We can see that all text is now in lower case and numbers have been removed.
text = tm_map(text,removePunctuation)            # Removing Punctuations
text = tm_map(text,removeWords,stopwords())      #to remove common words
text = tm_map(text,stemDocument)                 #to convert words back to root words.
text = tm_map(text,stripWhitespace)              #to remove white spaces
text = tm_map(text,removeWords,stopwords())      #to remove common words
as.character(text[[1]])
text = tm_map(text,stemDocument)                 #to convert words back to root words.
as.character(text[[1]])
full$review = gsub(full$review, pattern = '<br />', replacement = ' ')
text = VCorpus(VectorSource(full$review))        # Creating a Corpus of reviews
text = tm_map(text,content_transformer(tolower))  # Converting to lower case
as.character(text[[1]])
text = tm_map(text,removeNumbers)                # Removing numbers
as.character(text[[1]])
# We can see that all text is now in lower case and numbers have been removed.
text = tm_map(text,removePunctuation)  # Removing Punctuations
as.character(text[[1]])
text = tm_map(text,removeWords,stopwords())      #to remove common words
as.character(text[[1]])
text = tm_map(text,stemDocument)                 #to convert words back to root words.
as.character(text[[1]])
text = tm_map(text,stripWhitespace)              #to remove white spaces
as.character(text[[1]])
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
View(BigramTokenizer)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtm1 = DocumentTermMatrix(text)
View(dtm1)
dtm1[["ncol"]][[1]]
View(dtm1)
dtm1 = removeSparseTerms(dtm1,0.95)
View(dtm1)
dtm1 = DocumentTermMatrix(text)
dtm1 = removeSparseTerms(dtm1,0.95)
## this is dtm for 1 words. we will find the more important words and just use those
dtm2 = DocumentTermMatrix(text, control = list(tokenize = BigramTokenizer))
dtm2 = removeSparseTerms(dtm2,0.99)
## similarly, dtm of 2 words
dtm3 = DocumentTermMatrix(text, control = list(tokenize = TrigramTokenizer))
dtm3 = removeSparseTerms(dtm3,0.999)
##dtm for 3 words
dataset1 = as.data.frame(as.matrix(dtm1))
dataset2 = as.data.frame(as.matrix(dtm2))
dataset3 = as.data.frame(as.matrix(dtm3))
View(dataset1)
View(dataset2)
View(dataset3)
#for dtm1
dataset_counts1 = as.data.frame(colSums(dataset1))
View(dataset_counts1)
dataset_counts1$word = rownames(dataset_counts1)
View(dataset_counts1)
colnames(dataset_counts1) = c("count","word")
View(dataset_counts1)
dataset_counts1 = dataset_counts1[c(2,1)]
View(dataset_counts1)
dataset_counts1 = dataset_counts1 %>% arrange(-count)
View(dataset_counts1)
#for dtm2
dataset_counts2 = as.data.frame(colSums(dataset2))
dataset_counts2$word = rownames(dataset_counts2)
colnames(dataset_counts2) = c("count","word")
dataset_counts2 = dataset_counts2[c(2,1)]
dataset_counts2 = dataset_counts2 %>% arrange(-count)
#for dtm3
dataset_counts3 = as.data.frame(colSums(dataset3))
dataset_counts3$word = rownames(dataset_counts3)
colnames(dataset_counts3) = c("count","word")
dataset_counts3 = dataset_counts3[c(2,1)]
dataset_counts3= dataset_counts3 %>% arrange(-count)
head(dataset_counts1,20)
head(dataset_counts2,20)
head(dataset_counts3,20)
#Pre processing
final_dataset_words = bind_rows(dataset_counts1,dataset_counts2,dataset_counts3)
final_dataset = as.data.frame(cbind(dataset1,dataset2,dataset3))
dataset_train = final_dataset[1:25000,]
dataset_test = final_dataset[25001:50000,]
View(dataset_train)
dataset_train$y_pred = word_train$sentiment
dataset_train$y_pred = as.factor(dataset_train$y_pred)
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = 'binomial')
pred = predict(m1,newdata = dataset_test)
pred
type(dataset_train$y_pred)
typeof(dataset_train$y_pred)
dataset_train$y_pred = as.factor(dataset_train$y_pred)
typeof(dataset_train$y_pred)
summary(dataset_train$y_pred)
str(dataset_train$y_pred)
head(dataset_train$y_pred)
head(dataset_train$y_pred,n=20)
head(dataset_train$y_pred,n=100)
pred = predict(m1,newdata = dataset_test,family = 'binomial')
pred
pred = predict(m1,newdata = dataset_test,type = 'class')
pred = predict(m1,newdata = dataset_test,type = "response")
pred
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = binomial(link = logit))
pred = predict(m1,newdata = dataset_test,type = "response")
pred
dataset_train$y_pred = word_train$sentiment
dataset_train$y_pred = as.factor(dataset_train$y_pred)
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = 'binomial')
pred = predict(m1,newdata = dataset_test,type = "response")
probs <- exp(pred)/(1+exp(pred))
probs
length(probs < 0.5)
length(probs < 0.4)
length(probs < 0.3)
length(probs < 0.1)
length(probs < 0.0)
length(probs > 0.5)
length(probs)
length(which(probs > 0.5))
length(which(probs < 0.3))
length(which(probs < 0.4))
length(which(probs < 0.5))
length(which(probs < 0.51))
#pred = predict(m1,newdata = dataset_test,type = "response")
#probs <- exp(pred)/(1+exp(pred))
#gives you probability that y=1 for each observation
probs <- exp(predict(m1, type = "response" , newdata=dataset_test))
probs
