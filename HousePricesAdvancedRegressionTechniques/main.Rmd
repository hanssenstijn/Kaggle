---
title: "House Prices"
author: "Stijn Hanssen"
date: "1 November 2018"
output: html_document
---

### Clear working space
First step is to make sure that the working space is empty.
```{r}
# clear all variables
rm(list=ls()) 
```

### Load packages
Loading the required packages for specific functions. If the packaged aren't present, they will be automatically be downloaded.
```{r}
if (!require("pacman")) suppressPackageStartupMessages(install.packages("pacman"))
pacman::p_load("stringr","GLDEX","RSEIS","DescTools","caret","plyr","Metrics")
```

### Set working directory
Choose the path were the data-sets are stored
```{r}
DATA.DIR <- "~/GitHub/Kaggle/HousePricesAdvancedRegressionTechniques"
setwd(DATA.DIR)
# See which files are in WD
list.files() 
```

### Import data
```{r}
train <- read.csv("train.csv",stringsAsFactors = F)
test <- read.csv("test.csv", stringsAsFactors =  F)

dim(train)
dim(test)

str(train)

# Remove the target variable not found in test set
SalePrice = train$SalePrice 
train$SalePrice = NULL

# Combine data sets
full_data = rbind(train,test)

# Convert character columns to factor, filling NA values with "missing"
for (col in colnames(full_data)){
  if (typeof(full_data[,col]) == "character"){
    new_col = full_data[,col]
    new_col[is.na(new_col)] = "missing"
    full_data[col] = as.factor(new_col)
  }
}

# Separate out our train and test sets
train = full_data[1:nrow(train),]
train$SalePrice = SalePrice  
test = full_data[(nrow(train)+1):nrow(full_data),]

summary(train)

```
### NA's
None of the numeric variables contain negative values so encoding the NA's as a negative number is a simple way to convert them to numeric while making it easy to tell which values are actually NA.
```{r}
# Fill remaining NA values with -1
train[is.na(train)] = -1
test[is.na(test)] = -1
```

### Features affect home sales prices

```{r}
for (col in colnames(train)){
    if(is.numeric(train[,col])){
        if( abs(cor(train[,col],train$SalePrice)) > 0.5){
            print(col)
            print( cor(train[,col],train$SalePrice) )
        }
    }
}
```

```{r}
for (col in colnames(train)){
    if(is.numeric(train[,col])){
        if( abs(cor(train[,col],train$SalePrice)) < 0.1){
            print(col)
            print( cor(train[,col],train$SalePrice) )
        }
    }
}
```

```{r}
cors = cor(train[ , sapply(train, is.numeric)])
high_cor = which(abs(cors) > 0.6 & (abs(cors) < 1))
rows = rownames(cors)[((high_cor-1) %/% 38)+1]
cols = colnames(cors)[ifelse(high_cor %% 38 == 0, 38, high_cor %% 38)]
vals = cors[high_cor]

cor_data = data.frame(cols=cols, rows=rows, correlation=vals)
cor_data
```

### Distributions of the numeric variables with density plots
```{r}
for (col in colnames(train)){
  if(is.numeric(train[,col])){
    plot(density(train[,col]), main=col)
  }
}
```

### Predictive modeling
Combine the GrLivArea and the TotalBsmtSF, for train and test. Combine the baths. Remove ID.
```{r}
# Add variable that combines above grade living area with basement sq footage
train$total_sq_footage = train$GrLivArea + train$TotalBsmtSF
test$total_sq_footage = test$GrLivArea + test$TotalBsmtSF

# Add variable that combines above ground and basement full and half baths
train$total_baths = train$BsmtFullBath + train$FullBath + (0.5 * (train$BsmtHalfBath + train$HalfBath))
test$total_baths = test$BsmtFullBath + test$FullBath + (0.5 * (test$BsmtHalfBath + test$HalfBath))

# Remove Id since it should have no value in prediction
train$Id = NULL    
test$Id = NULL
```

Create the control object and tuning variable grid. Caret optimizes root mean squared error for regression by default, so if we want to optimize for RMSLE we should pass in a custom summary function via our caret control object. The R package "Metrics" has a function for computing RMSLE so we can use that to compute the performance metric inside our custom summary function.
```{r}
# Create custom summary function in proper format for caret
custom_summary = function(data, lev = NULL, model = NULL){
    out = rmsle(data[, "obs"], data[, "pred"])
    names(out) = c("rmsle")
    out
}

# Create control object
control = trainControl(method = "cv",  # Use cross validation
                        number = 5,     # 5-folds
                        summaryFunction = custom_summary,                                        allowParallel=T
)

# Create grid of tuning parameters
grid = expand.grid(nrounds=c(100, 200, 400, 800), # Test 4 values for boosting rounds
                    max_depth= c(4, 6),           # Test 2 values for tree depth
                    eta=c(0.1, 0.05, 0.025),      # Test 3 values for learning rate
                    gamma= c(0.1), 
                    colsample_bytree = c(1), 
                    min_child_weight = c(1),
                    subsample=c(0,0.2,0.4,0.6))
```

### Train model
```{r}
set.seed(12)

xgb_tree_model =  train(SalePrice~.,      # Predict SalePrice using all features
                        data=train,
                        method="xgbTree",
                        trControl=control, 
                        tuneGrid=grid, 
                        metric="rmsle",     # Use custom performance metric
                        maximize = FALSE)   # Minimize the metric
```

### Check the results.
```{r}
xgb_tree_model$results

xgb_tree_model$bestTune
```

### Which variables most important to the model
```{r}
varImp(xgb_tree_model)
```

### Predict
```{r}
test_predictions = predict(xgb_tree_model, newdata=test)
head(test_predictions)
```

### Save the outcome
```{r}
submission = read.csv("sample_submission.csv")
submission$SalePrice = test_predictions
write.csv(submission, "home_prices_xgb_sub1.csv", row.names=FALSE)
```



